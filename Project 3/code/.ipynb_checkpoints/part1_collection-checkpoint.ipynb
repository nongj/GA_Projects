{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b82ca11",
   "metadata": {},
   "source": [
    "# Project FitNut - Classification Modelling via Natural Language Processing\n",
    "\n",
    "<img src = \" ../image/cover.png\" alt = \"cover\"/>\n",
    "\n",
    "([*Source*](https://surgesr.com/wp-content/uploads/2021/09/surge-new-logo.png))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b53ecd",
   "metadata": {},
   "source": [
    "# Contents  \n",
    "\n",
    "## Part 1\n",
    "### [Overview](#overview)\n",
    "### [Problem Statement](#problemstatement)\n",
    "### [Method(ology)](#methodology)\n",
    "- [Import Libraries](#importlibraries)\n",
    "- [Model Framework](#modelframework)\n",
    "\n",
    "### [Data Collection](#datacollection)\n",
    "- [Fitness](#fitcollection)\n",
    "- [Nutrition](#nutcollection)\n",
    "\n",
    "## Part 2 (see Notebook 2)  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1f0c3",
   "metadata": {},
   "source": [
    "<div id=\"overview\"></div>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The prevalence of Covid-19 has accelerated the digital transformation journey of traditional brick-and-mortar businesses on a global scale. SURGE - an elite private gym concept under the Core Collective group specializing in curated fitness/wellness programmes - is exploring a new business unit that focuesses on a tailored dual fitness-and-nutrition concept as membership rates for their physical gym at 79 Anson Road have dwindled. Before deep-diving in, SURGE plans to study latest trends and grasp ground sentiments on fitness/nutrition through analyzing two relevant subreddit threads: [*r/bodyweightfitness*](https://www.reddit.com/r/bodyweightfitness/) and [*r/EatCheapAndHealthy*](https://www.reddit.com/r/EatCheapAndHealthy/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da91c42",
   "metadata": {},
   "source": [
    "<div id=\"problemstatement\"></div>\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "A blanket approach was adopted in downloading 2,000 threads from the *bodyweightfitness* and *EatCheapAndHealthy* subreddits (i.e. posts not distinguished by respective subreddits). However, the fitness and nutrition portfolios are handled by two different teams in SURGE. As the hired Data Science consultant, develop a classification model to determine which of the abovementioned subreddits a given thread originates from. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab0d42",
   "metadata": {},
   "source": [
    "<div id=\"methodology\"></div>\n",
    "\n",
    "### Method(ology)\n",
    "\n",
    "The relevant libraries are systematically imported and underlying rationale for the production model discussed. \n",
    "\n",
    "<div id=\"importlibraries\"></div>\n",
    "\n",
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f701d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Collectiong/Scraping Modules\n",
    "import requests\n",
    "\n",
    "#NLP Modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Modelling Modules\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b1597",
   "metadata": {},
   "source": [
    "<div id=\"modelframework\"></div>\n",
    "\n",
    "##### Model Framework\n",
    "\n",
    "The goal is to develop a binary classification model. To ensure holistic decision-making in production modelling, we consider a varied set of possible models, each with their respective mechanisms:   \n",
    "\n",
    "    a) Logistic Regression - Classifies an observation based on a relatively simple discriminative algorithm by means of establishing a probabilistic boundary between binary classes (above P(x) = class 1, below P(x) = class 2).    \n",
    "\n",
    "    b) KNeighborsClassifier (KNN)- Classifies an observation based on the properties of \"nearest neighbours\" typically measured by euclidean/manhattan distance across vector space.  \n",
    "\n",
    "    c) Multinomial Naive Bayes - Classification technique based on (a series of) conditional probabilities, whereby inherent features of an observation would contribute to its eventual calculated class. Unlike KNN, Naive Bayes assumes that observations are independent of one another. Multinomial Naive Bayes is selected over Gaussian and Bernoulli as the dataset involves discrete data which is not normally distributed and the predictor variable (i.e. subreddit title + selftext) is not binary in nature.  \n",
    "\n",
    "    d) Random Forest - Ensemble classification algorithm comprising multiple decision trees which predicts classes based on the \"wisdom of crowds\"; the assemblage of models (trees) operating as a community (crowd) will inform/condition all individual constituent models (tree).  \n",
    "\n",
    "Similarly, in evaluating model performance, we consider various scoring metrices to ensure the production model is robust:  \n",
    "\n",
    "    i) Accuracy - Perhaps the first scoring indicator most will review since it is arguably the most intuitive, derived by [(TP + TN) / (TP + FP + TN + FN)], gives the ratio of correct predictions to total predictions. However, accuracy in itself is not adequate as it does not inform on model effectiveness/precision, not to mention that accuracy can be artificially modified simply by adjusting the threshold to achieve a biased outcome.  \n",
    "\n",
    "    ii) F1-score - Defined as the harmonic mean of precision and recall, follows the formula [2 * (Precision * Recall) / (Precision + Recall)], where Precision is [TP / (TP + FP)] and Recall (i.e. sensitivity) is [TP / (TP + FN)]. Unlike classification problems where false negatives are unacceptable (i.e. prioritize sensitivity such as in the case of identifying potential terrorists or chronic/fatal diseases) or false positives are intolerable (i.e. prioritize specificity such as in the case of an individual being declared positive receiving punishment), in the context of this project, there is no positive/negative in this sense as both binary classes are merely stating subreddit origin of a given thread. Hence, we strive for a balanced proxy instead by examining the F1-score as a scoring metric, which not only gives weight to the percentage of true positives over the total positives in the data but serves as an indicator for confidence of predicted positives as well. The F1-score seeks to optimize both precision and recall simultaneously.  \n",
    "\n",
    "    iii) ROC AUC - Bringing it all together, we analyze the ROC AUC score as well, which equates to the area under the ROC probability curve for the True Positive Rate against the False Positive Rate. By doing so, we ascertain if the extent of false positives and false negatives are effectively minimized, in which case returning a score of close to 1. Conversely, for a (theoretically) completely flawed model, the ROC AUC score returns a 0.5, which depicts the worst case scenario where the model has no discrimination capacity to distinguish between both binary classes.   \n",
    "\n",
    "For all scoring metrices, we strive for a benchmark of 90% (i.e. >= 0.9). The intention of setting this benchmark is to attain an equilibrium between reducing classification errors which would result in time/effort required to manually reroute subreddit posts, and ensuring that the production model is not overfitted.  \n",
    "\n",
    "\n",
    "*TP = True Positives (Post predicted as belonging to bodyweightfitness subreddit and indeed belonging to bodyweightfitness subreddit)*  \n",
    "*TN = True Negatives (Post predicted as belonging to EatCheapAndHealthy subreddit and inteed belonging to EatCheapAndHealthy subreddit)*   \n",
    "*FP = False Positives (Post predicted as belonging to bodyweightfitness subreddit but actually under EatCheapAndHealthy subreddit)*  \n",
    "*FN = False Negatives (Post predicted as belonging to EatCheapAndHealthy subreddit but actually under bodyweightfitness subreddit)*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757d7c4",
   "metadata": {},
   "source": [
    "<div id=\"datacollection\"></div>\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "The models will be trained using ~2,000 posts from the *bodyweightfitness* and *EatCheapAndHealthy* subreddits. While this figure may seem excessive given that 1,000 posts should suffice in developing the model, the larger sample size is to provide buffer for invalid observations (e.g. duplicate posts, non-text posts). Additionally, a larger sample size will improve validity of results by virtue of a more encompassing train/test dataset.  \n",
    "\n",
    "<div id=\"fitcollection\"></div>\n",
    "\n",
    "##### Fitness Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70bf7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_url = 'https://api.pushshift.io/reddit/search/submission?subreddit=bodyweightfitness' # URL for fitness subreddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09e3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from SOF (https://www.reddit.com/r/pushshift/comments/bfc2m1/capping_at_1000_posts/)\n",
    "n = 0 # number of posts, initialized at zero\n",
    "last = '' # Used to cut off scrapping\n",
    "fit_posts = []  # Empty list to append fitness posts subsequently\n",
    "\n",
    "while n < 1000: # As the current Pushshift API limit is 100, we will have to create a loop to obtain 1,000 posts\n",
    "    req_fit = requests.get('{}&before={}'.format(fit_url,last)) # Scrap the subject URL\n",
    "    fit_data = req_fit.json() # Fit scrapped data using json\n",
    "    for post in fit_data['data']:\n",
    "        fit_posts.append(post) # Add scrapped post to consolidated list\n",
    "        n += 1 # Increase post counter by 1\n",
    "    last = int(post['created_utc']) # Set the cut off using time frequency for number of posts collected \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b47032",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df = pd.DataFrame(fit_posts) # Convert scrapped data into pd dataframe format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "852ed6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df = fit_df[['title','selftext','subreddit']] # Extract only relevant rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80057ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f607231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      1000 non-null   object\n",
      " 1   selftext   994 non-null    object\n",
      " 2   subreddit  1000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n"
     ]
    }
   ],
   "source": [
    "fit_df.info() # Generally no issues, with 1,000 posts scrapped and in the correct str dtype\n",
    "                # Address missing values during data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cbb1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which door frame pull-up bar?</td>\n",
       "      <td>Hi I've never trained pull-ups before and now ...</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having trouble gaining weight</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pull-up negatives</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long to see Results?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't have weights</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Upper back strengthening</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Question about calisthenics skills periodization</td>\n",
       "      <td>First of all,I'm sorry for my bad english\\nSo,...</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wrist and sternum pain are killing my gains</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Help me buy a assistance band.</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A good tip I picked up for high-frequency trai...</td>\n",
       "      <td>Hey everyone,\\n\\nI know there's a lot of inter...</td>\n",
       "      <td>bodyweightfitness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                      Which door frame pull-up bar?   \n",
       "1                      Having trouble gaining weight   \n",
       "2                                  Pull-up negatives   \n",
       "3                           How long to see Results?   \n",
       "4                               I don't have weights   \n",
       "5                           Upper back strengthening   \n",
       "6   Question about calisthenics skills periodization   \n",
       "7        Wrist and sternum pain are killing my gains   \n",
       "8                     Help me buy a assistance band.   \n",
       "9  A good tip I picked up for high-frequency trai...   \n",
       "\n",
       "                                            selftext          subreddit  \n",
       "0  Hi I've never trained pull-ups before and now ...  bodyweightfitness  \n",
       "1                                          [removed]  bodyweightfitness  \n",
       "2                                          [removed]  bodyweightfitness  \n",
       "3                                          [removed]  bodyweightfitness  \n",
       "4                                          [removed]  bodyweightfitness  \n",
       "5                                          [removed]  bodyweightfitness  \n",
       "6  First of all,I'm sorry for my bad english\\nSo,...  bodyweightfitness  \n",
       "7                                          [removed]  bodyweightfitness  \n",
       "8                                          [removed]  bodyweightfitness  \n",
       "9  Hey everyone,\\n\\nI know there's a lot of inter...  bodyweightfitness  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_df.head(10) # To address [removed] selftext during data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c516dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_df.duplicated().sum() # Noticeable number of duplicate posts which will likely be deleted during cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f77ea",
   "metadata": {},
   "source": [
    "<div id=\"nutcollection\"></div>\n",
    "\n",
    "##### Nutrition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f655ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_url = 'https://api.pushshift.io/reddit/search/submission?subreddit=EatCheapAndHealthy' # URL for nutrition subreddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a9179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "nut_posts = []\n",
    "\n",
    "while n < 1000:\n",
    "    req_nut = requests.get('{}&before={}'.format(nut_url,last))\n",
    "    nut_data = req_nut.json()\n",
    "    for post in nut_data['data']:\n",
    "        nut_posts.append(post)\n",
    "        n += 1\n",
    "    last = int(post['created_utc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629ce7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_df = pd.DataFrame(nut_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "844ec59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_df = nut_df[['title','selftext','subreddit']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84a6b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_df.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c46872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      1000 non-null   object\n",
      " 1   selftext   1000 non-null   object\n",
      " 2   subreddit  1000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n"
     ]
    }
   ],
   "source": [
    "nut_df.info() # Generally no issues, with 1,000 posts scrapped and in the correct str dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2786c853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Smoothie Diet: 21 Day Rapid Weight Loss Pr...</td>\n",
       "      <td>This smoothie diet 21 day rapid weight loss pr...</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Guide About Tosh Trek</td>\n",
       "      <td></td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Colorado alternative to Aldi</td>\n",
       "      <td>I moved to CO a few years ago, and I still hav...</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOW TO LOSE WEIGHT TIPS AND TRICKS</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This weeks theme ingredient is... Pumpkin! Wha...</td>\n",
       "      <td>Our next key ingredient is **Pumpkin!** Let us...</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Healthy meals advice</td>\n",
       "      <td>Hi everyone,\\n\\nI'm looking for meal ideas. He...</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fruit Salad.</td>\n",
       "      <td></td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Accidentally got grapefruit instead of oranges...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Buy organic A2 Cow Ghee Online in Delhi</td>\n",
       "      <td></td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I’ve inherited many,many meatballs. What can I...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>EatCheapAndHealthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Smoothie Diet: 21 Day Rapid Weight Loss Pr...   \n",
       "1                         Full Guide About Tosh Trek   \n",
       "2                       Colorado alternative to Aldi   \n",
       "3                 HOW TO LOSE WEIGHT TIPS AND TRICKS   \n",
       "4  This weeks theme ingredient is... Pumpkin! Wha...   \n",
       "5                               Healthy meals advice   \n",
       "6                                       Fruit Salad.   \n",
       "7  Accidentally got grapefruit instead of oranges...   \n",
       "8            Buy organic A2 Cow Ghee Online in Delhi   \n",
       "9  I’ve inherited many,many meatballs. What can I...   \n",
       "\n",
       "                                            selftext           subreddit  \n",
       "0  This smoothie diet 21 day rapid weight loss pr...  EatCheapAndHealthy  \n",
       "1                                                     EatCheapAndHealthy  \n",
       "2  I moved to CO a few years ago, and I still hav...  EatCheapAndHealthy  \n",
       "3                                          [removed]  EatCheapAndHealthy  \n",
       "4  Our next key ingredient is **Pumpkin!** Let us...  EatCheapAndHealthy  \n",
       "5  Hi everyone,\\n\\nI'm looking for meal ideas. He...  EatCheapAndHealthy  \n",
       "6                                                     EatCheapAndHealthy  \n",
       "7                                          [removed]  EatCheapAndHealthy  \n",
       "8                                                     EatCheapAndHealthy  \n",
       "9                                          [removed]  EatCheapAndHealthy  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nut_df.head(10) # To address [removed] and possibly null-value selftext during data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bcb8ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nut_df.duplicated().sum() # Noticeable number of duplicate posts which will likely be deleted during cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0f3d4",
   "metadata": {},
   "source": [
    "Having successfully collected the desired number of posts from both subreddits, we combine both into a single dataframe for further processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7849200",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitnut_df = pd.concat([fit_df, nut_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a987cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitnut_df.shape # Correct total number of subreddit posts (2,000) and columns (3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d27c25a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      2000 non-null   object\n",
      " 1   selftext   1994 non-null   object\n",
      " 2   subreddit  2000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "fitnut_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf2cc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export combined scrapped dataframe\n",
    "##fitnut_df.to_csv('../data/fitnut_scrapped.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a82b5",
   "metadata": {},
   "source": [
    "This concludes the current notebook (Part 1). In the following notebook (Part 2), we will perform data cleaning and EDA/visualizations, before moving to the formal modelling stage.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
